{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305538ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egermani/miniforge3/envs/workEnv/lib/python3.9/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n"
     ]
    }
   ],
   "source": [
    "from pyneurovault import api\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join as opj\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b87cc",
   "metadata": {},
   "source": [
    "# Fetching NeuroVault "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743c497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egermani/miniforge3/envs/workEnv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7,8,9,10,12,14,18,19,23,24,26,27,28,33,39,40,41,43,47,50,51,52,54,55,56,62,63,64,67,151,152,154,1153,1156,1157,1158,1167,1168,1170,1171,1172,1175,1178,1179,1180,1181,1182,1183) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Using NeuroVault API, get metadata about all images available in NeuroVault\n",
    "if not os.path.exists('../data/original/neurovault_api_request_images.csv'):\n",
    "    df_images = api.get_images(limit=1000) # Search NeuroVault database for all images and \n",
    "    # load metadata in a Pandas DataFrame\n",
    "    df_images.to_csv('../data/original/neurovault_api_request_images.csv')\n",
    "else:\n",
    "    print('Dataframe already exists.')\n",
    "    df_images = pd.read_csv('../data/original/neurovault_api_request_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc5c92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egermani/miniforge3/envs/workEnv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (24,26,32,40,51,52,53,54,55,56,57,58,59,60,61,62,63,67,68,69,70,71,72,73,74,76,77,81,89,97,98,99,100,101,102,103,104) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Using NeuroVault API, get metadata about all collections available in NeuroVault\n",
    "if not os.path.exists('../data/original/neurovault_api_request_collections.csv'):\n",
    "\tdf_collec = api.get_collections(limit = 10000) # Search NeuroVault database for all collections and \n",
    "    #load metadata in a Pandas DataFrame\n",
    "\tdf_collec.to_csv('../data/original/neurovault_api_request_collections.csv')\n",
    "else:\n",
    "\tdf_collec = pd.read_csv('../data/original/neurovault_api_request_collections.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26b4442",
   "metadata": {},
   "source": [
    "# Selection of NeuroVault dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e672f58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuroVault dataset: 28532 images\n"
     ]
    }
   ],
   "source": [
    "error_images = [51429, 51430, 67793, 110481, 110482, 110483, 110484, 110487, 110488, 110489, 110490, 113776, 115008, \n",
    "133840, 133841, 133842, 133843, 133844, 133845, 133846, 133847, 133848, 133849, 133850, 306407, 306408, 306409,\n",
    "306410, 306411, 306412, 306413, 360472, 394776, 394777, 394778, 394779, 350627, 1076,\n",
    "13165, 14181,14182,14183,14184,14185,14186,14187,14188,14189,14190,14191,14192,14193,14194,14195,14196,14197,14198,\n",
    "14199,14200,14272,15995,15996,15999,16000,18511,18512,18513,18514,18515,18516,18517,18518,18519,18520,18521,18522,\n",
    "18523,18524,18525,18643,18644,18669,18670,20696,23569,23687,24091,28871,28872,28886,28937,28938,28957,28958,39055,\n",
    "39879, 43806, 44483, 44485, 50315, 51432, 51433,52580,52620,52927,53140,53142,53144,53146,53148,53150,53163,53229,\n",
    "53493,53495,53497,56157,58233,58234,58235,58236,58237,58344,58348,59158,59159,60072,60330,61079,61482,61483,61484,\n",
    "61485,61486,61487,61488,61489,62502,62704,62705,63130,64373,64374,64376,64391,64766,65080,65085,65140,65215,65221,\n",
    "65227,65417,65418,65426,67916,67917,68058,68087,68088,68089,68090,68091,100342,124714,124718,124719,124720,124721,\n",
    "124722,124723,124724,124725,124726,124736,124755,124756,124757,124758,124759,124760,124761,124911,124912,128534,\n",
    "128535,128536,128537,128538,128539,128540,128541,128542,128543,128544,128545,128546,128547,128548,128549,128550,\n",
    "128551,128552,128553,135787,135788,135789,304534,304536,381325,381326,381327,381328,381329,381330,381331,381332,\n",
    "381333,381334,381335,381336,381337,381338,381339,381340,381341,381342,381343,381344,406412,406417,513197,513207] \n",
    "# Error when trying to download these images --> Corrupted ?\n",
    "\n",
    "\n",
    "# Retrieval of selected images\n",
    "id_list = []\n",
    "if os.path.exists(opj('../data/original/NeuroVault_dataset/neurovault_dataset.txt')):\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj('../data/original/NeuroVault_dataset/neurovault_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            id_list.append(line[:-1])\n",
    "    file.close()\n",
    "\n",
    "else:\n",
    "    # If file doesn't exist, make selection based on metadata using dataframe.\n",
    "    for i in df_images.index:\n",
    "        if (df_images.loc[i]['analysis_level'] == 'group' or df_images.loc[i]['analysis_level'] == 'single-subject')\\\n",
    "        and df_images.loc[i]['is_valid'] and not df_images.loc[i]['not_mni'] \\\n",
    "        and (df_images.loc[i]['map_type'] == 'T map' or df_images.loc[i]['map_type'] == 'Z map') \\\n",
    "        and not df_images.loc[i]['is_thresholded'] \\\n",
    "        and df_images.loc[i]['modality'] == 'fMRI-BOLD' and df_images.loc[i]['image_type'] == 'statistic_map' \\\n",
    "        and not (\"Set\" in df_images.loc[i]['file'] and \"mean\" in df_images.loc[i]['file']) \\\n",
    "        and df_images.loc[i]['image_id'] not in error_images:\n",
    "            id_list.append(str(df_images.loc[i]['image_id']))\n",
    "\n",
    "    subject_level_df = df_images[df_images['image_id'].isin([int(i) for i in id_list])]\n",
    "    subject_level_df = subject_level_df[subject_level_df['collection_id'].isin(df_collec['collection_id']\\\n",
    "                                        [~df_collec['name'].str.contains('temporary')].tolist())]\n",
    "\n",
    "    id_list = [str(i) for i in subject_level_df['image_id'].tolist()]\n",
    "    \n",
    "    # Store selected ids in txt file to avoid the time consuming task of selection. \n",
    "    with open(opj('../data/original/NeuroVault_dataset/neurovault_dataset.txt'), \"w\") as file:\n",
    "        for ids in id_list:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "print('NeuroVault dataset:', len(id_list), 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dda8c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into TRAIN and TEST \n",
    "# If file available, read the IDs \n",
    "if os.path.exists(opj('../data/original/NeuroVault_dataset', 'train_neurovault_dataset.txt')) and \\\n",
    "    os.path.exists(opj('../data/original/NeuroVault_dataset', 'test_neurovault_dataset.txt')):\n",
    "    \n",
    "    global_train_df = []\n",
    "    with open(opj('../data/original/NeuroVault_dataset', 'train_neurovault_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            global_train_df.append(line[:-1])\n",
    "    file.close() \n",
    "    \n",
    "    global_test_df = []\n",
    "    with open(opj('../data/original/NeuroVault_dataset', 'test_neurovault_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            global_test_df.append(line[:-1])\n",
    "    file.close() \n",
    "\n",
    "# If files not available, split the data and store the ids in files\n",
    "else: \n",
    "    \n",
    "    global_df = []\n",
    "    with open(opj('../data/original/NeuroVault_dataset', 'neurovault_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            global_df.append(line[:-1])\n",
    "    file.close() \n",
    "    \n",
    "    global_train_df, global_test_df = train_test_split(global_df, \n",
    "                                                       test_size = 0.2, train_size = 0.8)\n",
    "    \n",
    "    \n",
    "    with open(opj('../data/original/NeuroVault_dataset/train_neurovault_dataset.txt'), \"w\") as file:\n",
    "        for ids in global_train_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(opj('../data/original/NeuroVault_dataset/test_neurovault_dataset.txt'), \"w\") as file:\n",
    "        for ids in global_test_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba89fd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 22772\n",
      "Test set: 5760\n"
     ]
    }
   ],
   "source": [
    "print('Training set:', len(global_train_df))\n",
    "print('Test set:', len(global_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a9ab7",
   "metadata": {},
   "source": [
    "# Selection of HCP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a290be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCP dataset: 18070 images\n"
     ]
    }
   ],
   "source": [
    "# Finding IDs of HCP images stored in NeuroVault\n",
    "if not os.path.exists('../data/original/HCP_dataset/hcp_dataset.txt'): \n",
    "    df_hcp = df_images[df_images['collection_id']==4337]\n",
    "    \n",
    "    id_list = df_hcp['image_id'].tolist()\n",
    "    with open(opj('../data/original/HCP_dataset/hcp_dataset.txt'), \"w\") as file:\n",
    "        for ids in id_list:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "else: \n",
    "    df_hcp = df_images[df_images['collection_id']==4337]\n",
    "    \n",
    "    id_list = []\n",
    "    with open(opj('../data/original/HCP_dataset/hcp_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            id_list.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "print('HCP dataset:', len(id_list), 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88648230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding useful metadata and store them in the label file\n",
    "if not os.path.exists('../data/original/HCP_dataset/hcp_dataset_labels.csv'):\n",
    "    \n",
    "    df_hcp['subject'] = [0 for i in range(18070)]\n",
    "\n",
    "    for i in df_hcp['file'].to_list():\n",
    "        df_hcp['subject'][df_hcp['file']==i] = df_hcp['name'][df_hcp['file']==i].tolist()[0][0:6]\n",
    "\n",
    "    df_hcp = df_hcp[['image_id', 'subject', 'cognitive_paradigm_cogatlas', 'task', 'contrast_definition']]\n",
    "    df_hcp.rename(columns = {'contrast_definition':'contrast'}, inplace = True)\n",
    "\n",
    "    df_hcp.to_csv('../data/original/HCP_dataset/hcp_dataset_labels.csv')\n",
    "    \n",
    "else:\n",
    "    df_hcp = pd.read_csv('../data/original/HCP_dataset/hcp_dataset_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db63b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 9017\n",
      "Valid set: 9053\n"
     ]
    }
   ],
   "source": [
    "# TRAIN / VALID SPLIT \n",
    "# If file already exists, read ids\n",
    "subs = 'hcp_dataset'\n",
    "data_dir = '../data/original/HCP_dataset'\n",
    "if os.path.exists(opj(data_dir, 'train_' + str(subs) + '.txt')):\n",
    "    train_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    valid_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            valid_df.append(line[:-1])\n",
    "    file.close()\n",
    "\n",
    "# If file doesn't exist, perform split\n",
    "else:\n",
    "    global_df = []\n",
    "    with open(opj(data_dir, f'{subs}.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            global_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    global_subs = np.unique(df_hcp['subject'].tolist())\n",
    "    print(len(global_subs))\n",
    "    \n",
    "    train_subs, valid_subs = train_test_split(global_subs, \n",
    "                                            test_size = 0.5, train_size = 0.5)\n",
    "    \n",
    "    print(len(train_subs))\n",
    "    print(len(valid_subs))\n",
    "    \n",
    "    train_df = df_hcp['image_id'][df_hcp['subject'].isin(train_subs)]\n",
    "    valid_df = df_hcp['image_id'][df_hcp['subject'].isin(valid_subs)]\n",
    "        \n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in train_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in valid_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "print('Training set:', len(train_df))\n",
    "print('Valid set:', len(valid_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09252b",
   "metadata": {},
   "source": [
    "## HCP global subsets of different sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb82438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "Sample 200\n",
      "Train: 4590\n",
      "Valid: 4591\n",
      "Sample 100\n",
      "Train: 2300\n",
      "Valid: 2300\n",
      "Sample 50\n",
      "Train: 1150\n",
      "Valid: 1150\n"
     ]
    }
   ],
   "source": [
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin([int(i) for i in train_df])].tolist())\n",
    "valid_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin([int(i) for i in valid_df])].tolist())\n",
    "print(len(train_subs))\n",
    "for n in [200, 100, 50]:\n",
    "    print('Sample', n)\n",
    "    if not os.path.exists(opj(data_dir, 'train_hcp_dataset_'+ str(n) + '.txt')):\n",
    "        train_subs = np.random.choice(train_subs, n, replace=False)\n",
    "        print(len(train_subs))\n",
    "        train_df = df_hcp['image_id'][df_hcp['subject'].isin(train_subs)]\n",
    "        print('Train:', len(train_df))\n",
    "        \n",
    "        valid_subs = np.random.choice(valid_subs, n, replace=False)\n",
    "        print(len(valid_subs))\n",
    "        valid_df = df_hcp['image_id'][df_hcp['subject'].isin(valid_subs)]\n",
    "        print('Valid:', len(valid_df))\n",
    "        \n",
    "        with open(opj(data_dir, f'train_hcp_dataset_{n}.txt'), \"w\") as file:\n",
    "            for ids in train_df:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "        file.close()\n",
    "\n",
    "        with open(opj(data_dir, f'valid_hcp_dataset_{n}.txt'), \"w\") as file:\n",
    "            for ids in valid_df:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "        file.close()\n",
    "        \n",
    "    else:\n",
    "        train_df = []\n",
    "        with open(opj(data_dir, f'train_hcp_dataset_{n}.txt'), \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                train_df.append(line[:-1])\n",
    "        file.close()\n",
    "        print('Train:', len(train_df))\n",
    "        \n",
    "        valid_df = []\n",
    "        with open(opj(data_dir, f'valid_hcp_dataset_{n}.txt'), \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                valid_df.append(line[:-1])\n",
    "        file.close()\n",
    "        print('Valid:', len(valid_df))\n",
    "        \n",
    "\n",
    "    df_hcp.to_csv(opj(data_dir, f'hcp_dataset_{n}_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1055c877",
   "metadata": {},
   "source": [
    "## Small HCP dataset for One contrast task classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4625f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2355\n",
      "Valid set: 2361\n"
     ]
    }
   ],
   "source": [
    "# TRAIN / VALID SPLIT \n",
    "# If file already exists, read ids\n",
    "subs = 'small_hcp_dataset'\n",
    "data_dir = '../data/original/HCP_dataset'\n",
    "df_hcp = pd.read_csv('../data/original/HCP_dataset/hcp_dataset_labels.csv')\n",
    "\n",
    "if not os.path.exists(opj(data_dir, f'{subs}_labels.csv')):\n",
    "    small_hcp_df = df_hcp[df_hcp['contrast'].isin(['2BKPLACE', 'FACES', 'PUNISH', 'REL' ,'RH', 'STORY' ,'TOM'])]\n",
    "    small_hcp_df.to_csv(opj(data_dir, f'{subs}_labels.csv'))\n",
    "\n",
    "else:\n",
    "    small_hcp_df = pd.read_csv(opj(data_dir, f'{subs}_labels.csv'))\n",
    "\n",
    "if os.path.exists(opj(data_dir, 'train_' + str(subs) + '.txt')):\n",
    "    train_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    valid_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            valid_df.append(line[:-1])\n",
    "    file.close()\n",
    "\n",
    "# If file doesn't exist, find images\n",
    "else:\n",
    "    train_id = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'train_hcp_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_id.append(int(line[:-1]))\n",
    "    file.close()\n",
    "    \n",
    "    valid_id = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'valid_hcp_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            valid_id.append(int(line[:-1]))\n",
    "    file.close()\n",
    "    \n",
    "    train_df = small_hcp_df['image_id'][small_hcp_df['image_id'].isin(train_id)]\n",
    "    \n",
    "    valid_df = small_hcp_df['image_id'][small_hcp_df['image_id'].isin(valid_id)]\n",
    "    \n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in train_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in valid_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "print('Training set:', len(train_df))\n",
    "print('Valid set:', len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f729d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393\n",
      "Sample 200\n",
      "Train: 1198\n",
      "Valid: 1200\n",
      "Sample 100\n",
      "Train: 598\n",
      "Valid: 600\n",
      "Sample 50\n",
      "Train: 300\n",
      "Valid: 300\n"
     ]
    }
   ],
   "source": [
    "train_subs = np.unique(small_hcp_df['subject'][small_hcp_df['image_id'].isin([int(i) for i in train_df])].tolist())\n",
    "valid_subs = np.unique(small_hcp_df['subject'][small_hcp_df['image_id'].isin([int(i) for i in valid_df])].tolist())\n",
    "print(len(train_subs))\n",
    "for n in [200, 100, 50]:\n",
    "    print('Sample', n)\n",
    "    if not os.path.exists(opj(data_dir, 'train_small_hcp_dataset_'+ str(n) + '.txt')):\n",
    "        train_subs = np.random.choice(train_subs, n, replace=False)\n",
    "        train_df = small_hcp_df['image_id'][small_hcp_df['subject'].isin(train_subs)]\n",
    "        print('Train:', len(train_df))\n",
    "        \n",
    "        valid_subs = np.random.choice(valid_subs, n, replace=False)\n",
    "        valid_df = small_hcp_df['image_id'][small_hcp_df['subject'].isin(valid_subs)]\n",
    "        print('Valid:', len(valid_df))\n",
    "        \n",
    "        with open(opj(data_dir, f'train_small_hcp_dataset_{n}.txt'), \"w\") as file:\n",
    "            for ids in train_df:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "        file.close()\n",
    "\n",
    "        with open(opj(data_dir, f'valid_small_hcp_dataset_{n}.txt'), \"w\") as file:\n",
    "            for ids in valid_df:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "        file.close()\n",
    "        \n",
    "    else:\n",
    "        train_df = []\n",
    "        with open(opj(data_dir, f'train_small_hcp_dataset_{n}.txt'), \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                train_df.append(line[:-1])\n",
    "        file.close()\n",
    "        print('Train:', len(train_df))\n",
    "        \n",
    "        valid_df = []\n",
    "        with open(opj(data_dir, f'valid_small_hcp_dataset_{n}.txt'), \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                valid_df.append(line[:-1])\n",
    "        file.close()\n",
    "        print('Valid:', len(valid_df))\n",
    "        \n",
    "\n",
    "    small_hcp_df.to_csv(opj(data_dir, f'small_hcp_dataset_{n}_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e7ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# KFOLD CV \n",
    "import random\n",
    "import math\n",
    "data_dir = '../data/original/HCP_dataset'\n",
    "df_hcp = pd.read_csv('../data/original/HCP_dataset/hcp_dataset_labels.csv')\n",
    "df_small_hcp = pd.read_csv('../data/original/HCP_dataset/small_hcp_dataset_labels.csv')\n",
    "\n",
    "subs = 'hcp_dataset_50'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds_50 = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "train_folds_50 = [[subs for subs in train_subs if subs not in test_folds_50[i]] for i in range(5)]\n",
    "\n",
    "train_folds_50_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_50[i])].tolist() for i in range(5)]\n",
    "test_folds_50_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_50[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(train_folds_50_id[i]))\n",
    "        for ids in train_folds_50_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(test_folds_50_id[i]))\n",
    "        for ids in test_folds_50_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_50_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_50[i])].tolist() for i in range(5)]\n",
    "test_folds_50_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_50[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_small_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(train_folds_50_id_small[i]))\n",
    "        for ids in train_folds_50_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_small_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(test_folds_50_id_small[i]))\n",
    "        for ids in test_folds_50_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset_100'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_50 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_100 = [test_folds_50[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_100 = [[subs for subs in train_subs if subs not in test_folds_100[i]] for i in range(5)]\n",
    "\n",
    "train_folds_100_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_100[i])].tolist() for i in range(5)]\n",
    "test_folds_100_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_100[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_100_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_100_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_100_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_100[i])].tolist() for i in range(5)]\n",
    "test_folds_100_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_100[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_small_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_100_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_small_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_100_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset_200'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_100 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_200 = [test_folds_100[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_200 = [[subs for subs in train_subs if subs not in test_folds_200[i]] for i in range(5)]\n",
    "\n",
    "train_folds_200_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_200[i])].tolist() for i in range(5)]\n",
    "test_folds_200_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_200[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_200_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_200_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_200_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_200[i])].tolist() for i in range(5)]\n",
    "test_folds_200_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_200[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_small_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_200_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_small_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_200_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_200 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_all = [test_folds_200[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_all = [[subs for subs in train_subs if subs not in test_folds_all[i]] for i in range(5)]\n",
    "\n",
    "train_folds_all_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_all[i])].tolist() for i in range(5)]\n",
    "test_folds_all_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_all[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_all_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_all_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_all_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_all[i])].tolist() for i in range(5)]\n",
    "test_folds_all_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_all[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_perf_small_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_all_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_perf_small_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_all_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37811de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "920\n",
      "230\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n",
      "240\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# KFOLD CV \n",
    "import random\n",
    "import math\n",
    "data_dir = '../data/original/HCP_dataset'\n",
    "df_hcp = pd.read_csv('../data/original/HCP_dataset/hcp_dataset_labels.csv')\n",
    "df_small_hcp = pd.read_csv('../data/original/HCP_dataset/small_hcp_dataset_labels.csv')\n",
    "\n",
    "subs = 'hcp_dataset_50'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds_50 = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "train_folds_50 = [[subs for subs in train_subs if subs not in test_folds_50[i]] for i in range(5)]\n",
    "\n",
    "train_folds_50_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_50[i])].tolist() for i in range(5)]\n",
    "test_folds_50_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_50[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(train_folds_50_id[i]))\n",
    "        for ids in train_folds_50_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(test_folds_50_id[i]))\n",
    "        for ids in test_folds_50_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_50_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_50[i])].tolist() for i in range(5)]\n",
    "test_folds_50_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_50[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_small_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(train_folds_50_id_small[i]))\n",
    "        for ids in train_folds_50_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_small_hcp_dataset_50_fold_{i}.txt'), \"w\") as file:\n",
    "        print(len(test_folds_50_id_small[i]))\n",
    "        for ids in test_folds_50_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset_100'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_50 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_100 = [test_folds_50[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_100 = [[subs for subs in train_subs if subs not in test_folds_100[i]] for i in range(5)]\n",
    "\n",
    "train_folds_100_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_100[i])].tolist() for i in range(5)]\n",
    "test_folds_100_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_100[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_100_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_100_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_100_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_100[i])].tolist() for i in range(5)]\n",
    "test_folds_100_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_100[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_small_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_100_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_small_hcp_dataset_100_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_100_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset_200'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_100 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_200 = [test_folds_100[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_200 = [[subs for subs in train_subs if subs not in test_folds_200[i]] for i in range(5)]\n",
    "\n",
    "train_folds_200_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_200[i])].tolist() for i in range(5)]\n",
    "test_folds_200_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_200[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_200_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_200_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_200_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_200[i])].tolist() for i in range(5)]\n",
    "test_folds_200_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_200[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_small_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_200_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_small_hcp_dataset_200_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_200_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "subs = 'hcp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)][~df_hcp['subject'].isin([item for sublist in test_folds_200 for item in sublist])].tolist())\n",
    "random.shuffle(train_subs)\n",
    "n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "test_folds_all = [test_folds_200[i] + test_folds[i] for i in range(5)]\n",
    "train_subs = np.unique(df_hcp['subject'][df_hcp['image_id'].isin(train_df)].tolist())\n",
    "train_folds_all = [[subs for subs in train_subs if subs not in test_folds_all[i]] for i in range(5)]\n",
    "\n",
    "train_folds_all_id = [df_hcp['image_id'][df_hcp['subject'].isin(train_folds_all[i])].tolist() for i in range(5)]\n",
    "test_folds_all_id = [df_hcp['image_id'][df_hcp['subject'].isin(test_folds_all[i])].tolist() for i in range(5)]\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_all_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_all_id[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "train_folds_all_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(train_folds_all[i])].tolist() for i in range(5)]\n",
    "test_folds_all_id_small = [df_small_hcp['image_id'][df_small_hcp['subject'].isin(test_folds_all[i])].tolist() for i in range(5)]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    with open(opj(data_dir, f'train_hp_small_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in train_folds_all_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    with open(opj(data_dir, f'test_hp_small_hcp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for ids in test_folds_all_id_small[i]:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d4cb2",
   "metadata": {},
   "source": [
    "# Selection of BrainPedia datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241e66a",
   "metadata": {},
   "source": [
    "## Large BrainPedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b74532",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bp = df_images[df_images['collection_id']==1952]\n",
    "\n",
    "tags_to_rm = []\n",
    "for t in np.unique(df_bp['tags']).tolist():\n",
    "    if len(df_bp[df_bp['tags']==t]) <= 30:\n",
    "        tags_to_rm.append(t)\n",
    "\n",
    "df_bp = df_bp[~df_bp['tags'].isin(tags_to_rm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffdb84d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainPedia dataset: 6448 images\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('../data/original/BrainPedia_dataset/bp_dataset.txt'):\n",
    "    id_list = df_bp['image_id'].tolist()\n",
    "    with open(opj('../data/original/BrainPedia_dataset/bp_dataset.txt'), \"w\") as file:\n",
    "        for ids in id_list:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "else: \n",
    "    id_list = []\n",
    "    with open(opj('../data/original/BrainPedia_dataset/bp_dataset.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            id_list.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "print('BrainPedia dataset:', len(id_list), 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89761a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/original/BrainPedia_dataset/bp_dataset_labels.csv'):    \n",
    "    subjects = [name.split('_')[-1] for name in df_bp['name'].tolist()]\n",
    "    studies = [name.split('_')[0] for name in df_bp['name'].tolist()]\n",
    "    tasks = [name.split('_')[1] for name in df_bp['name'].tolist()]\n",
    "\n",
    "    df_bp = df_bp[['image_id', 'tags', 'name']]\n",
    "\n",
    "    df_bp['subject'] = subjects\n",
    "    df_bp['study'] = studies\n",
    "    df_bp['task'] = tasks\n",
    "    \n",
    "    df_bp.to_csv('../data/original/BrainPedia_dataset/bp_dataset_labels.csv')\n",
    "    \n",
    "else:\n",
    "    df_bp = pd.read_csv('../data/original/BrainPedia_dataset/bp_dataset_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5179d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3179\n",
      "Valid set: 3269\n"
     ]
    }
   ],
   "source": [
    "# TRAIN / VALID SPLIT \n",
    "# If file already exists, read ids\n",
    "subs = 'bp_dataset'\n",
    "data_dir = '../data/original/BrainPedia_dataset'\n",
    "if os.path.exists(opj(data_dir, 'train_' + str(subs) + '.txt')):\n",
    "    train_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    valid_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            valid_df.append(line[:-1])\n",
    "    file.close()\n",
    "\n",
    "# If file doesn't exist, perform split\n",
    "else:\n",
    "    global_df = []\n",
    "    with open(opj(data_dir, f'{subs}.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            global_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    train_df = pd.DataFrame()\n",
    "    valid_df = pd.DataFrame()\n",
    "    \n",
    "    for study in np.unique(df_bp['study'].tolist()):\n",
    "        subjects = np.unique(df_bp['subject'][df_bp['study']==study])\n",
    "        if len(subjects) > 1:\n",
    "            train_subs, valid_subs = train_test_split(subjects, \n",
    "                                                test_size = 0.5, train_size = 0.5)\n",
    "            train_df = train_df.append(df_bp[df_bp['study']==study]\\\n",
    "                                                 [df_bp['subject'].isin(train_subs)])\n",
    "\n",
    "            valid_df = valid_df.append(df_bp[df_bp['study']==study]\\\n",
    "                                                 [df_bp['subject'].isin(valid_subs)])\n",
    "            \n",
    "        else: \n",
    "            train_df = train_df.append(df_bp[df_bp['study']==study]\\\n",
    "                                                 [df_bp['subject'].isin(subjects)])\n",
    "\n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in train_df['image_id']:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in valid_df['image_id']:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "print('Training set:', len(train_df))\n",
    "print('Valid set:', len(valid_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f87ba",
   "metadata": {},
   "source": [
    "## Small BrainPedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db74ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 917\n",
      "Valid set: 927\n"
     ]
    }
   ],
   "source": [
    "subs = 'small_bp_dataset'\n",
    "data_dir = '../data/original/BrainPedia_dataset'\n",
    "\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_bp_dataset.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(line[:-1])\n",
    "file.close()\n",
    "\n",
    "valid_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_bp_dataset.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        valid_df.append(line[:-1])\n",
    "file.close()\n",
    "if not os.path.exists(opj(data_dir, f'{subs}_labels.csv')):\n",
    "    studies = df_bp.groupby(['study']).nunique()['subject'] > 20\n",
    "\n",
    "    small_bp_studies = [studies.index.tolist()[i] for i in range(len(studies)) if studies[i]]\n",
    "\n",
    "    small_bp_df = df_bp[df_bp['study'].isin(small_bp_studies)]\n",
    "    \n",
    "    train_small_bp = small_bp_df[small_bp_df['image_id'].isin([int(i) for i in train_df])]\n",
    "    valid_small_bp = small_bp_df[small_bp_df['image_id'].isin([int(i) for i in valid_df])]\n",
    "    \n",
    "    train_small_bp_2 = pd.DataFrame()\n",
    "    valid_small_bp_2 = pd.DataFrame()\n",
    "    \n",
    "    for study in np.unique(small_bp_df['study'].tolist()):\n",
    "        print('Study:', study)\n",
    "        subjects = np.unique(train_small_bp['subject'][train_small_bp['study']==study])\n",
    "        subjects_to_keep = np.random.choice(subjects, 10, replace=False)\n",
    "        train_small_bp_2 = train_small_bp_2.append(small_bp_df[small_bp_df['study']==study]\\\n",
    "                                             [small_bp_df['subject'].isin(subjects_to_keep)])\n",
    "        \n",
    "        subjects = np.unique(valid_small_bp['subject'][valid_small_bp['study']==study])\n",
    "        subjects_to_keep = np.random.choice(subjects, 10, replace=False)\n",
    "        valid_small_bp_2 = valid_small_bp_2.append(small_bp_df[small_bp_df['study']==study]\\\n",
    "                                             [small_bp_df['subject'].isin(subjects_to_keep)])\n",
    "        \n",
    "    \n",
    "    train_df = train_small_bp_2['image_id']\n",
    "    valid_df = valid_small_bp_2['image_id']\n",
    "    \n",
    "    small_bp_df_2 = train_small_bp_2.append(valid_small_bp_2)\n",
    "    \n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in train_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"w\") as file:\n",
    "        for ids in valid_df:\n",
    "            file.write(str(ids))\n",
    "            file.write('\\n')\n",
    "    file.close()\n",
    "        \n",
    "    small_bp_df_2.to_csv(opj(data_dir, f'{subs}_labels.csv'))\n",
    "\n",
    "else:\n",
    "    small_bp_df_2 = pd.read_csv(opj(data_dir, f'{subs}_labels.csv'))\n",
    "    \n",
    "    train_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            train_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "    valid_df = []\n",
    "    # If file containing ids of selected maps exists, read it and store the list.\n",
    "    with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            valid_df.append(line[:-1])\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "print('Training set:', len(train_df))\n",
    "print('Valid set:', len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ba9a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/original/BrainPedia_dataset'\n",
    "df_bp = pd.read_csv('../data/original/BrainPedia_dataset/bp_dataset_labels.csv')\n",
    "df_small_bp = pd.read_csv('../data/original/BrainPedia_dataset/small_bp_dataset_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a25ef69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small BP fold 0: 734 train, 183 test\n",
      "Small BP fold 1: 734 train, 183 test\n",
      "Small BP fold 2: 729 train, 188 test\n",
      "Small BP fold 3: 736 train, 181 test\n",
      "Small BP fold 4: 735 train, 182 test\n",
      "BP fold 0: 2597 train, 582 test\n",
      "BP fold 1: 2600 train, 579 test\n",
      "BP fold 2: 2593 train, 586 test\n",
      "BP fold 3: 2617 train, 562 test\n",
      "BP fold 4: 2614 train, 565 test\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "data_dir = '../data/original/BrainPedia_dataset'\n",
    "df_bp = pd.read_csv('../data/original/BrainPedia_dataset/bp_dataset_labels.csv')\n",
    "df_small_bp = pd.read_csv('../data/original/BrainPedia_dataset/small_bp_dataset_labels.csv')\n",
    "\n",
    "subs = 'small_bp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "\n",
    "train_folds_sbp_id = []\n",
    "test_folds_sbp_id = []\n",
    "test_folds_sbp = []\n",
    "train_folds_sbp = []\n",
    "\n",
    "for s, studies in enumerate(np.unique(df_small_bp['study'].tolist())):\n",
    "    train_subs = np.unique(df_small_bp['subject'][df_small_bp['study']==studies][df_small_bp['image_id'].isin(train_df)].tolist())\n",
    "\n",
    "    random.shuffle(train_subs)\n",
    "    n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "    test_folds_sbp.append([train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)])\n",
    "    train_folds_sbp.append([[subs for subs in train_subs if subs not in test_folds_sbp[s][i]] for i in range(5)])\n",
    "\n",
    "    train_folds_sbp_id.append([df_small_bp['image_id'][df_small_bp['study']==studies][df_small_bp['subject'].isin(train_folds_sbp[s][i])].tolist() for i in range(5)])\n",
    "    test_folds_sbp_id.append([df_small_bp['image_id'][df_small_bp['study']==studies][df_small_bp['subject'].isin(test_folds_sbp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    fold_length_tr = 0\n",
    "    fold_length_te = 0\n",
    "    with open(opj(data_dir, f'train_perf_small_bp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_small_bp['study'].tolist()))):\n",
    "            fold_length_tr += len(train_folds_sbp_id[s][i])\n",
    "            for ids in train_folds_sbp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    with open(opj(data_dir, f'test_perf_small_bp_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_small_bp['study'].tolist()))):\n",
    "            fold_length_te += len(test_folds_sbp_id[s][i])\n",
    "            for ids in test_folds_sbp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    print(f'Small BP fold {i}: {fold_length_tr} train, {fold_length_te} test')\n",
    "\n",
    "\n",
    "subs = 'bp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'train_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "\n",
    "train_folds_bp_id = []\n",
    "test_folds_bp_id = []\n",
    "train_folds_bp = []\n",
    "test_folds_bp = []\n",
    "\n",
    "# For each study of BrainPedia\n",
    "for s, studies in enumerate(np.unique(df_bp['study'].tolist())):\n",
    "    # Check if this study is also part of small Brainpedia \n",
    "    # If so, find the list of subjects that are already included in the folds of small BrainPedia \n",
    "    # These will be the base of the folds of BrainPedia for this study\n",
    "    if studies in np.unique(df_small_bp['study'].tolist()):\n",
    "        s_bis = np.unique(df_small_bp['study'].tolist()).tolist().index(studies)\n",
    "        # Train subs corresponds to the subjects that are part of the study but not in small brainpedia \n",
    "        train_subs = np.unique(df_bp['subject'][df_bp['study']==studies][df_bp['image_id'].isin(train_df)][~df_bp['subject'].isin([item for sublist in test_folds_sbp[s_bis] for item in sublist])].tolist())\n",
    "        # Shuffle these subjects \n",
    "        random.shuffle(train_subs)\n",
    "        n_val = math.floor(len(train_subs)/5)\n",
    "        # If there are more than 5 subjects --> do a classical CV\n",
    "        if len(train_subs) >= 5:\n",
    "            test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "            test_folds_bp.append([test_folds_sbp[s_bis][i] + test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([train_folds_sbp[s_bis][i] + [subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "        else:\n",
    "            test_folds = [[train_subs[i]] for i in range(len(train_subs))] + [[] for i in range(5-len(train_subs))]\n",
    "            test_folds_bp.append([test_folds_sbp[s_bis][i] + test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([train_folds_sbp[s_bis][i] + [subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "    else:\n",
    "        train_subs = np.unique(df_bp['subject'][df_bp['study']==studies][df_bp['image_id'].isin(train_df)].tolist())\n",
    "        random.shuffle(train_subs)\n",
    "        n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "        if len(train_subs) >= 5:\n",
    "            test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "            test_folds_bp.append([test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([[subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "        else:\n",
    "            test_folds = [[train_subs[i]] for i in range(len(train_subs))] + [[] for i in range(5-len(train_subs))]\n",
    "            test_folds_bp.append([test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([[subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "for i in range(5):\n",
    "    fold_length_tr = 0\n",
    "    fold_length_te = 0\n",
    "    with open(opj(data_dir, f'train_perf_bp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_bp['study'].tolist()))):\n",
    "            fold_length_tr += len(train_folds_bp_id[s][i])\n",
    "            for ids in train_folds_bp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    with open(opj(data_dir, f'test_perf_bp_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_bp['study'].tolist()))):\n",
    "            fold_length_te += len(test_folds_bp_id[s][i])\n",
    "            for ids in test_folds_bp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    print(f'BP fold {i}: {fold_length_tr} train, {fold_length_te} test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1504ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small BP fold 0: 745 train, 182 test\n",
      "Small BP fold 1: 743 train, 184 test\n",
      "Small BP fold 2: 736 train, 191 test\n",
      "Small BP fold 3: 735 train, 192 test\n",
      "Small BP fold 4: 749 train, 178 test\n",
      "BP fold 0: 2673 train, 596 test\n",
      "BP fold 1: 2668 train, 601 test\n",
      "BP fold 2: 2663 train, 606 test\n",
      "BP fold 3: 2675 train, 594 test\n",
      "BP fold 4: 2684 train, 585 test\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "data_dir = '../data/original/BrainPedia_dataset'\n",
    "df_bp = pd.read_csv('../data/original/BrainPedia_dataset/bp_dataset_labels.csv')\n",
    "df_small_bp = pd.read_csv('../data/original/BrainPedia_dataset/small_bp_dataset_labels.csv')\n",
    "\n",
    "subs = 'small_bp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "\n",
    "train_folds_sbp_id = []\n",
    "test_folds_sbp_id = []\n",
    "test_folds_sbp = []\n",
    "train_folds_sbp = []\n",
    "\n",
    "for s, studies in enumerate(np.unique(df_small_bp['study'].tolist())):\n",
    "    train_subs = np.unique(df_small_bp['subject'][df_small_bp['study']==studies][df_small_bp['image_id'].isin(train_df)].tolist())\n",
    "\n",
    "    random.shuffle(train_subs)\n",
    "    n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "    test_folds_sbp.append([train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)])\n",
    "    train_folds_sbp.append([[subs for subs in train_subs if subs not in test_folds_sbp[s][i]] for i in range(5)])\n",
    "\n",
    "    train_folds_sbp_id.append([df_small_bp['image_id'][df_small_bp['study']==studies][df_small_bp['subject'].isin(train_folds_sbp[s][i])].tolist() for i in range(5)])\n",
    "    test_folds_sbp_id.append([df_small_bp['image_id'][df_small_bp['study']==studies][df_small_bp['subject'].isin(test_folds_sbp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "for i in range(5):\n",
    "    fold_length_tr = 0\n",
    "    fold_length_te = 0\n",
    "    with open(opj(data_dir, f'train_hp_small_bp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_small_bp['study'].tolist()))):\n",
    "            fold_length_tr += len(train_folds_sbp_id[s][i])\n",
    "            for ids in train_folds_sbp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    with open(opj(data_dir, f'test_hp_small_bp_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_small_bp['study'].tolist()))):\n",
    "            fold_length_te += len(test_folds_sbp_id[s][i])\n",
    "            for ids in test_folds_sbp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    print(f'Small BP fold {i}: {fold_length_tr} train, {fold_length_te} test')\n",
    "\n",
    "\n",
    "subs = 'bp_dataset'\n",
    "train_df = []\n",
    "# If file containing ids of selected maps exists, read it and store the list.\n",
    "with open(opj(data_dir, 'valid_' + str(subs) + '.txt'), \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        train_df.append(int(line[:-1]))\n",
    "file.close()\n",
    "\n",
    "train_folds_bp_id = []\n",
    "test_folds_bp_id = []\n",
    "train_folds_bp = []\n",
    "test_folds_bp = []\n",
    "\n",
    "# For each study of BrainPedia\n",
    "for s, studies in enumerate(np.unique(df_bp['study'].tolist())):\n",
    "    # Check if this study is also part of small Brainpedia \n",
    "    # If so, find the list of subjects that are already included in the folds of small BrainPedia \n",
    "    # These will be the base of the folds of BrainPedia for this study\n",
    "    if studies in np.unique(df_small_bp['study'].tolist()):\n",
    "        s_bis = np.unique(df_small_bp['study'].tolist()).tolist().index(studies)\n",
    "        # Train subs corresponds to the subjects that are part of the study but not in small brainpedia \n",
    "        train_subs = np.unique(df_bp['subject'][df_bp['study']==studies][df_bp['image_id'].isin(train_df)][~df_bp['subject'].isin([item for sublist in test_folds_sbp[s_bis] for item in sublist])].tolist())\n",
    "        # Shuffle these subjects \n",
    "        random.shuffle(train_subs)\n",
    "        n_val = math.floor(len(train_subs)/5)\n",
    "        # If there are more than 5 subjects --> do a classical CV\n",
    "        if len(train_subs) >= 5:\n",
    "            test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "            test_folds_bp.append([test_folds_sbp[s_bis][i] + test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([train_folds_sbp[s_bis][i] + [subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "        else:\n",
    "            test_folds = [[train_subs[i]] for i in range(len(train_subs))] + [[] for i in range(5-len(train_subs))]\n",
    "            test_folds_bp.append([test_folds_sbp[s_bis][i] + test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([train_folds_sbp[s_bis][i] + [subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "    else:\n",
    "        train_subs = np.unique(df_bp['subject'][df_bp['study']==studies][df_bp['image_id'].isin(train_df)].tolist())\n",
    "        random.shuffle(train_subs)\n",
    "        n_val = math.floor(len(train_subs)/5)\n",
    "\n",
    "        if len(train_subs) >= 5:\n",
    "            test_folds = [train_subs[i:i + n_val].tolist() for i in range(0, len(train_subs), n_val)]\n",
    "            test_folds_bp.append([test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([[subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "        else:\n",
    "            test_folds = [[train_subs[i]] for i in range(len(train_subs))] + [[] for i in range(5-len(train_subs))]\n",
    "            test_folds_bp.append([test_folds[i] for i in range(5)])\n",
    "            train_folds_bp.append([[subs for subs in train_subs if subs not in test_folds_bp[s][i]] for i in range(5)])\n",
    "            train_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(train_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "            test_folds_bp_id.append([df_bp['image_id'][df_bp['study']==studies][df_bp['subject'].isin(test_folds_bp[s][i])].tolist() for i in range(5)])\n",
    "\n",
    "for i in range(5):\n",
    "    fold_length_tr = 0\n",
    "    fold_length_te = 0\n",
    "    with open(opj(data_dir, f'train_hp_bp_dataset_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_bp['study'].tolist()))):\n",
    "            fold_length_tr += len(train_folds_bp_id[s][i])\n",
    "            for ids in train_folds_bp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "\n",
    "    with open(opj(data_dir, f'test_hp_bp_fold_{i}.txt'), \"w\") as file:\n",
    "        for s in range(len(np.unique(df_bp['study'].tolist()))):\n",
    "            fold_length_te += len(test_folds_bp_id[s][i])\n",
    "            for ids in test_folds_bp_id[s][i]:\n",
    "                file.write(str(ids))\n",
    "                file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    print(f'BP fold {i}: {fold_length_tr} train, {fold_length_te} test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd50f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
